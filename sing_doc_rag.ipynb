{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e0db5f",
   "metadata": {},
   "source": [
    "# Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7446fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5e39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.llamaindex.ai/blog/improving-vector-search-reranking-with-postgresml-and-llamaindex'\n",
    "r = requests.get(url)\n",
    "s = BeautifulSoup(r.content, 'html.parser')\n",
    "p = s.find_all('p', class_='Text_text__zPO0D Text_text-size-16__PkjFu')\n",
    "\n",
    "p_list = []\n",
    "counter = 1\n",
    "for item in p:\n",
    "    content = {}\n",
    "    if item.text.strip() and (item.next_sibling and item.next_sibling.name != 'pre'):\n",
    "        content['content'] = item.text.strip()\n",
    "        content['id'] = f'paragraph_{counter}'\n",
    "        p_list.append(content)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9154f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sing_doc.json', 'w') as f:\n",
    "    json.dump(p_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e48ea8e",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecabc925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08ececc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de048a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b19772e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sing_doc.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "989a3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Create documents with metadata, filtering out empty paragraphs\n",
    "documents = [\n",
    "    Document(\n",
    "        text=item['content'].strip(), \n",
    "        metadata={\"id\": item['id']}\n",
    "    ) \n",
    "    for item in data if item['content'].strip()  # Filter out empty paragraphs\n",
    "]\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abe51819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of nodes: 16\n"
     ]
    }
   ],
   "source": [
    "# Split documents into nodes\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "print(\"len of nodes:\", len(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f1b7b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM model and embedding loaded\n"
     ]
    }
   ],
   "source": [
    "# Load LLM Model\n",
    "Settings.llm = Gemini(api_key=gemini_key, model='models/gemini-pro')\n",
    "Settings.embed_model = GeminiEmbedding(api_key=gemini_key, model='models/embedding-001')\n",
    "print('LLM model and embedding loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "356f0243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary and Vector Index loaded\n"
     ]
    }
   ],
   "source": [
    "# Vector Store Index\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "print('Summary and Vector Index loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "050de9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Query Engine\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_node=\"tree summarize\",\n",
    "    use_async=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e76b6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Query Engine\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=\"Useful for summarization questions related to any topic in Deep Learning paper\"\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for retrieving specific context from the Deep Learning paper.\"\n",
    ")\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[summary_tool, vector_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6cb5d244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for a summary of the document, which is a task related to summarization..\n",
      "\u001b[0mQuestion 1: What is the summary of the document?\n",
      "This document discusses the use of cross-encoders for reranking in search systems. Cross-encoders directly compare query-result pairs for similarity, making them effective for evaluating new, unseen data without the need for extensive user interaction data for fine-tuning. The document provides an example of implementing a simple reranking system using LlamaIndex and the PostgresML managed index, which handles storing, splitting, embedding, and querying documents. The example shows how to use the mixedbread-ai/mxbai-rerank-base-v1 model to rerank the top 100 results from a semantic search, resulting in more precise answers.\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What is the summary of the document?\"\n",
    "response1 = query_engine.query(question1)\n",
    "print(\"Question 1:\", question1)\n",
    "print(str(response1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c45260ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for key points in the document, which is related to summarization..\n",
      "\u001b[0mQuestion 2: What are the key points in the document? Which paragraph are you referring to?\n",
      "**Key Points:**\n",
      "\n",
      "* Search systems use keyword and semantic methods to match queries to content.\n",
      "* Reranking can improve result relevance, especially for new content.\n",
      "* Cross-encoders directly compare query-result pairs for similarity, making them effective for reranking.\n",
      "* Cross-encoders complement traditional reranking systems by addressing their limitations in deep text analysis.\n",
      "* PostgresML Managed Index can handle storing, splitting, embedding, and querying documents for reranking.\n",
      "\n",
      "**Paragraph:** 1, 2, 3, 4, 6, 10, 11, 12, 15\n"
     ]
    }
   ],
   "source": [
    "question2 = \"What are the key points in the document? Which paragraph are you referring to?\"\n",
    "response2 = query_engine.query(question2)\n",
    "print(\"Question 2:\", question2)\n",
    "print(str(response2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e395d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
